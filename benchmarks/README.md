# Benchmarks

This directory contains the performance regression infrastructure for LogWatcher.

## Directory layout

```
benchmarks/
  baseline/          # Committed baseline JSON files (one per benchmark class)
  compare.py         # Regression comparison script
  README.md          # This file
```

## ⚠️ Baseline status

The current baseline files are **synthetic placeholders** created because .NET SDK 10.0.101 was
not available in the authoring environment. They encode representative order-of-magnitude values
for each benchmark, not measured numbers. **Regenerate them** on a machine with the SDK installed
before relying on the regression gate for real decisions (see "Updating the baseline" below).

## What the baseline is

`benchmarks/baseline/` holds `*-report-full.json` files generated by BenchmarkDotNet.
Each file corresponds to one benchmark class. These files are committed to the repository
and serve as the reference point for regression detection in CI.

## Updating the baseline

Run whenever you make an intentional performance improvement (not after noise):

```bash
# Run benchmarks locally with the same short job used in CI
dotnet run --project LogWatcher.Benchmarks -c Release -- --job short --filter * --exporters json github

# Copy the new JSONs over the old baseline
Copy-Item BenchmarkDotNet.Artifacts\results\*-report-full.json benchmarks\baseline\

# Commit
git add benchmarks/baseline/
git commit -m "perf: update benchmark baseline after <description>"
```

**Do not update the baseline** after a run that shows noise or unexplained variance.
Only update after a deliberate, understood performance improvement.

## Regression threshold

The default threshold is **15%** (`--threshold 0.15`). This means a benchmark must be more
than 15% slower than baseline to be flagged as a regression.

To change it for a one-off check:

```bash
python benchmarks/compare.py \
  --baseline-dir benchmarks/baseline \
  --current-dir BenchmarkDotNet.Artifacts/results \
  --threshold 0.10   # 10% threshold
```

To change the CI threshold, edit the `--threshold` argument in `.github/workflows/ci.yml`
under the `Check for regressions` step.

## Running the comparison script locally

```bash
python benchmarks/compare.py \
  --baseline-dir benchmarks/baseline \
  --current-dir BenchmarkDotNet.Artifacts/results \
  --threshold 0.15
```

Exit code 0 = all benchmarks within threshold. Exit code 1 = at least one regression.

## CI job

The `perf-test` job in `.github/workflows/ci.yml` runs on every PR to `main`.
It uses `--job short` (≈1 warmup + 3 measured iterations) to keep runtime under 10 minutes.
Results are uploaded as the `benchmark-results` artifact for each run.
